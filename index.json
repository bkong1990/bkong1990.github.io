[{"authors":["Bin Kong","Shanhui Sun","Xin Wang","Qi Song","Shaoting Zhang"],"categories":null,"content":"","date":1537070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537070400,"objectID":"71843a95dcdd8ff7a9997f0c3f7ce495","permalink":"https://bkong1990.github.io/publication/miccai2018/","publishdate":"2018-09-16T00:00:00-04:00","relpermalink":"/publication/miccai2018/","section":"publication","summary":"Identification of invasive cancer in Whole Slide Images (WSIs) is crucial for tumor staging as well as treatment planning. However, the precise manual delineation of tumor regions is challenging, tedious and time-consuming. Thus, automatic invasive cancer detection in WSIs is of significant importance. Recently, Convolutional Neural Network (CNN) based approaches advanced invasive cancer detection. However, computation burdens of these approaches become barriers in clinical applications. In this work, we propose to detect invasive cancer employing a lightweight network in a fully convolution fashion without model ensembles. In order to improve the small network’s detection accuracy, we utilized the “soft labels” of a large capacity network to supervise its training process. Additionally, we adopt a teacher guided loss to help the small network better learn from the intermediate layers of the high capacity network. With this suite of approaches, our network is extremely efficient as well as accurate. The proposed method is validated on two large scale WSI datasets. Our approach is performed in an average time of 0.6 and 3.6 min per WSI with a single GPU on our gastric cancer dataset and CAMELYON16, respectively, about 5 times faster than Google Inception V3. We achieved an average FROC of   81.1%  and   85.6%  respectively, which are on par with Google Inception V3. The proposed method requires less high performance computing resources than state-of-the-art methods, which makes the invasive cancer diagnosis more applicable in the clinical usage.","tags":[],"title":"Invasive Cancer Detection Utilizing Compressed Convolutional Neural Network and Transfer Learning","type":"publication"},{"authors":["Bin Kong","Xin Wang","Zhongyu Li","Qi Song","Shaoting Zhang"],"categories":null,"content":"","date":1498363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498363200,"objectID":"88a08c4ab1545c588d17f754fad88c8d","permalink":"https://bkong1990.github.io/publication/ipmi2017/","publishdate":"2017-06-25T00:00:00-04:00","relpermalink":"/publication/ipmi2017/","section":"publication","summary":"Metastasis detection of lymph nodes in Whole-slide Images (WSIs) plays a critical role in the diagnosis of breast cancer. Automatic metastasis detection is a challenging issue due to the large variance of their appearances and the size of WSIs. Recently, deep neural networks have been employed to detect cancer metastases by dividing the WSIs into small image patches. However, most existing works simply treat these patches independently and do not consider the structural information among them. In this paper, we propose a novel deep neural network, namely Spatially Structured Network (Spatio-Net) to tackle the metastasis detection problem in WSIs. By integrating the Convolutional Neural Network (CNN) with the 2D Long-Short Term Memory (2D-LSTM), our Spatio-Net is able to learn the appearances and spatial dependencies of image patches effectively. Specifically, the CNN encodes each image patch into a compact feature vector, and the 2D-LSTM layers provide the classification results (i.e., normal or tumor), considering its dependencies on other relevant image patches. Moreover, a new loss function is designed to constrain the structure of the output labels, which further improves the performance. Finally, the metastasis positions are obtained by locating the regions with high tumor probabilities in the resulting accurate probability map. The proposed method is validated on hundreds of WSIs, and the accuracy is significantly improved, in comparison with a state-of-the-art baseline that does not have the spatial dependency constraint.","tags":[],"title":"Cancer metastasis detection via spatially structured deep network","type":"publication"},{"authors":["Bin Kong","Yiqiang Zhan","Min Shin","Thomas Denny","Shaoting Zhang"],"categories":null,"content":"","date":1475380800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475380800,"objectID":"c68cd01cb435f655e2de9a3977d3ee9a","permalink":"https://bkong1990.github.io/publication/miccai2016/","publishdate":"2016-10-02T00:00:00-04:00","relpermalink":"/publication/miccai2016/","section":"publication","summary":"Accurate measurement of left ventricular volumes and Ejection Fraction from cine MRI is of paramount importance to the evaluation of cardiovascular functions, yet it usually requires laborious and tedious work of trained experts to interpret them. To facilitate this procedure, numerous computer aided diagnosis (CAD) methods and tools have been proposed, most of which focus on the left or right ventricle segmentation. However, the identification of ES and ED frames from cardiac sequences is largely ignored, which is a key procedure in the automated workflow. This seemingly easy task is quite challenging, due to the requirement of high accuracy (i.e., precisely identifying specific frames from a sequence) and subtle differences among consecutive frames. Recently, with the rapid growth of annotated data and the increasing computational power, deep learning methods have been widely exploited in medical image analysis. In this paper, we propose a novel deep learning architecture, named as temporal regression network (TempReg-Net), to accurately identify specific frames from MRI sequences, by integrating the Convolutional Neural Network (CNN) with the Recurrent Neural Network (RNN). Specifically, a CNN encodes the spatial information of a cardiac sequence, and a RNN decodes the temporal information. In addition, we design a new loss function in our network to constrain the structure of predicted labels, which further improves the performance. Our approach is extensively validated on thousands of cardiac sequences and the average difference is merely 0.4 frames, comparing favorably with previous systems.","tags":[],"title":"Recognizing End-Diastole and End-Systole Frames via Deep Temporal Regression Network","type":"publication"}]